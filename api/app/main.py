from fastapi import FastAPI, HTTPException, Response
from pydantic import BaseModel
import joblib
import numpy as np
import logging
from typing import List
import uvicorn
import os
from datetime import datetime
from prometheus_client import Counter, Histogram, Gauge, generate_latest

# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Fraud Detection API - Banque Fluzz",
    description="API de d√©tection de fraude bancaire pour la n√©obanque Fluzz",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# M√©triques Prometheus
PREDICTIONS_TOTAL = Counter(
    'fraud_predictions_total', 
    'Nombre total de pr√©dictions',
    ['prediction_type', 'model_version']
)

PREDICTION_LATENCY = Histogram(
    'fraud_prediction_duration_seconds',
    'Temps de traitement des pr√©dictions'
)

MODEL_ACCURACY = Gauge(
    'fraud_detection_model_accuracy',
    'Pr√©cision actuelle du mod√®le'
)

API_REQUESTS = Counter(
    'api_requests_total',
    'Nombre total de requ√™tes API',
    ['method', 'endpoint', 'status']
)

# üëâ Nouvelles m√©triques custom pour Grafana
DETECTION_RATE = Gauge(
    'fraud_detection_rate',
    'Taux de d√©tection des fraudes'
)

FALSE_POSITIVE_RATE = Gauge(
    'fraud_false_positive_rate',
    'Taux de faux positifs'
)

DATA_DRIFT_SCORE = Gauge(
    'fraud_data_drift_score',
    'Score de d√©rive des donn√©es'
)

# Variables globales pour les mod√®les
model = None
scaler = None
model_metadata = None

def load_models():
    """Charge les mod√®les et scalers depuis les fichiers"""
    global model, scaler, model_metadata
    
    try:
        # Chemins vers les mod√®les (ajustez selon votre structure)
        base_path = os.path.join(os.path.dirname(__file__), "..", "..", "data", "processed")
        
        scaler_path = os.path.join(base_path, "scalers.pkl")
        metadata_path = os.path.join(base_path, "metadata.pkl")
        
        # Chargement du scaler
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            logger.info(f"Scaler charg√© depuis {scaler_path}")
        else:
            logger.warning(f"Scaler non trouv√© √† {scaler_path}")
        
        # Chargement des m√©tadonn√©es
        if os.path.exists(metadata_path):
            model_metadata = joblib.load(metadata_path)
            logger.info(f"M√©tadonn√©es charg√©es depuis {metadata_path}")
        else:
            logger.warning(f"M√©tadonn√©es non trouv√©es √† {metadata_path}")
        
        # Pour cette d√©mo, cr√©ons un mod√®le simple (en production, chargez votre meilleur mod√®le)
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.datasets import make_classification
        
        # Simulation d'un mod√®le entra√Æn√© pour les 30 features de creditcard
        X_sim, y_sim = make_classification(
            n_samples=1000, n_features=30, n_informative=15, 
            n_redundant=5, n_classes=2, random_state=42
        )
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_sim, y_sim)
        
        # Simulation d'une pr√©cision pour les m√©triques
        MODEL_ACCURACY.set(0.95)
        
        logger.info("Mod√®les charg√©s avec succ√®s")
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement des mod√®les: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """√âv√©nement de d√©marrage - charge les mod√®les"""
    success = load_models()
    if not success:
        logger.error("√âchec du chargement des mod√®les")
        # En production, vous pourriez vouloir arr√™ter l'application
        # raise Exception("Impossible de charger les mod√®les")
    
    # üëâ Initialisation des m√©triques avec des valeurs du projet r√©el
    DETECTION_RATE.set(0.75)  # Random Forest recall r√©el
    FALSE_POSITIVE_RATE.set(0.0001)  # 0.01% = 10/85295 transactions
    DATA_DRIFT_SCORE.set(0.05)  # Tr√®s faible d√©rive
    logger.info("M√©triques Prometheus initialis√©es")

class TransactionInput(BaseModel):
    """Sch√©ma d'entr√©e pour une transaction"""
    features: List[float]  # Les 30 features (V1-V28 + Time + Amount)
    transaction_id: str = None
    
    class Config:
        schema_extra = {
            "example": {
                "features": [1.0, -1.3598071336738, -0.0727811733098497] + [0.0] * 27,
                "transaction_id": "txn_123456789"
            }
        }

class PredictionOutput(BaseModel):
    """Sch√©ma de sortie pour la pr√©diction"""
    transaction_id: str
    is_fraud: bool
    fraud_probability: float
    confidence_score: float
    model_version: str
    timestamp: str
    
class BatchTransactionInput(BaseModel):
    """Sch√©ma pour les pr√©dictions en lot"""
    transactions: List[TransactionInput]

class HealthResponse(BaseModel):
    """R√©ponse du health check"""
    status: str
    timestamp: str
    model_loaded: bool
    scaler_loaded: bool
    version: str

@app.get("/api", response_model=dict)
async def root():
    """Endpoint racine"""
    API_REQUESTS.labels(method="GET", endpoint="/", status="200").inc()
    return {
        "message": "API Fraud Detection - Banque Fluzz",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/api/health", response_model=HealthResponse)
async def health_check():
    """Health check pour Kubernetes"""
    API_REQUESTS.labels(method="GET", endpoint="/health", status="200").inc()
    
    return HealthResponse(
        status="healthy" if (model is not None and scaler is not None) else "degraded",
        timestamp=datetime.utcnow().isoformat(),
        model_loaded=model is not None,
        scaler_loaded=scaler is not None,
        version="1.0.0"
    )

@app.post("/api/predict", response_model=PredictionOutput)
async def predict_fraud(transaction: TransactionInput):
    """Pr√©diction pour une transaction unique"""
    start_time = datetime.utcnow()
    
    try:
        # Validation des mod√®les
        if model is None or scaler is None:
            API_REQUESTS.labels(method="POST", endpoint="/predict", status="500").inc()
            raise HTTPException(
                status_code=500, 
                detail="Mod√®les non disponibles"
            )
        
        # Validation des features
        if len(transaction.features) != 30:
            API_REQUESTS.labels(method="POST", endpoint="/predict", status="400").inc()
            raise HTTPException(
                status_code=400, 
                detail="Exactement 30 features sont requises"
            )
        
        # Pr√©paration des donn√©es
        features_array = np.array(transaction.features).reshape(1, -1)
        
        # Normalisation (si scaler disponible)
        try:
            features_scaled = scaler.transform(features_array)
        except:
            features_scaled = features_array  # Fallback sans scaling
            logger.warning("Impossible d'appliquer le scaler, utilisation des features brutes")
        
        # Pr√©diction
        prediction = model.predict(features_scaled)[0]
        probabilities = model.predict_proba(features_scaled)[0]
        
        # Calcul de la confiance
        confidence = max(probabilities)
        fraud_probability = float(probabilities[1]) if len(probabilities) > 1 else 0.0
        
        # M√©triques
        prediction_type = "fraud" if prediction == 1 else "normal"
        PREDICTIONS_TOTAL.labels(
            prediction_type=prediction_type,
            model_version="1.0.0"
        ).inc()
        
        # Latence
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        PREDICTION_LATENCY.observe(processing_time)
        
        # G√©n√©ration d'un ID si non fourni
        txn_id = transaction.transaction_id or f"txn_{int(datetime.utcnow().timestamp())}"

        # üëâ Mise √† jour des nouvelles m√©triques
        # En production, il faudra calculer ces valeurs sur un batch ou via monitoring
        DETECTION_RATE.set(0.75)  # Random Forest recall r√©el
        FALSE_POSITIVE_RATE.set(0.0001)  # 0.01% fausses alertes
        DATA_DRIFT_SCORE.set(0.05)  # Faible d√©rive

        logger.info(
            f"Pr√©diction effectu√©e - ID: {txn_id}, "
            f"Fraude: {prediction}, "
            f"Confiance: {confidence:.3f}, "
            f"Temps: {processing_time:.3f}s"
        )
        
        API_REQUESTS.labels(method="POST", endpoint="/predict", status="200").inc()
        
        return PredictionOutput(
            transaction_id=txn_id,
            is_fraud=bool(prediction),
            fraud_probability=fraud_probability,
            confidence_score=float(confidence),
            model_version="1.0.0",
            timestamp=datetime.utcnow().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        API_REQUESTS.labels(method="POST", endpoint="/predict", status="500").inc()
        logger.error(f"Erreur lors de la pr√©diction: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@app.post("/api/batch_predict")
async def batch_predict(batch: BatchTransactionInput):
    """Pr√©dictions en lot pour am√©liorer les performances"""
    try:
        if len(batch.transactions) == 0:
            API_REQUESTS.labels(method="POST", endpoint="/batch_predict", status="400").inc()
            raise HTTPException(
                status_code=400,
                detail="Aucune transaction fournie"
            )
        
        if len(batch.transactions) > 100:
            API_REQUESTS.labels(method="POST", endpoint="/batch_predict", status="400").inc()
            raise HTTPException(
                status_code=400,
                detail="Maximum 100 transactions par lot"
            )
        
        results = []
        for transaction in batch.transactions:
            try:
                prediction_result = await predict_fraud(transaction)
                results.append(prediction_result.dict())
            except Exception as e:
                # En cas d'erreur sur une transaction, l'ignorer
                logger.error(f"Erreur sur transaction: {e}")
                continue
        
        API_REQUESTS.labels(method="POST", endpoint="/batch_predict", status="200").inc()
        
        return {
            "predictions": results,
            "count": len(results),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        API_REQUESTS.labels(method="POST", endpoint="/batch_predict", status="500").inc()
        logger.error(f"Erreur lors des pr√©dictions batch: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/metrics")
async def get_metrics():
    """Endpoint m√©triques pour Prometheus"""
    return Response(
        generate_latest(),
        media_type="text/plain"
    )

@app.get("/api/info")
async def get_info():
    """Informations sur l'API et le mod√®le"""
    API_REQUESTS.labels(method="GET", endpoint="/info", status="200").inc()
    
    return {
        "api": {
            "name": "Fraud Detection API",
            "version": "1.0.0",
            "description": "API de d√©tection de fraude pour la n√©obanque Fluzz"
        },
        "model": {
            "version": "1.0.0",
            "type": "RandomForestClassifier",
            "features_count": 30,
            "loaded": model is not None
        },
        "preprocessing": {
            "scaler_loaded": scaler is not None,
            "feature_names": [f"V{i}" for i in range(1, 29)] + ["Time", "Amount"]
        },
        "endpoints": {
            "predict": "/predict",
            "batch_predict": "/batch_predict",
            "health": "/health",
            "metrics": "/metrics",
            "docs": "/docs"
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )