"""
Module de Traitement des Donn√©es pour D√©tection de Fraude
=========================================================

Ce module contient toutes les fonctions de preprocessing et feature engineering
pour pr√©parer les donn√©es de transactions bancaires √† l'entra√Ænement ML.

Fonctionnalit√©s principales:
- Nettoyage et validation des donn√©es
- Feature engineering temporel et m√©tier
- Normalisation et mise √† l'√©chelle
- Gestion du d√©s√©quilibre des classes

Auteur: √âquipe Data Science - N√©obanque Fluzz
Version: 1.0
Date: Ao√ªt 2025
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import logging

from .utils import setup_logging, validate_dataset_quality, calculate_class_balance_metrics

# Configuration du logger
logger = setup_logging("INFO")

class DataPreprocessor:
    """
    Classe principale pour le preprocessing des donn√©es de transactions.
    
    Cette classe encapsule toutes les √©tapes de traitement des donn√©es,
    du nettoyage initial jusqu'√† la pr√©paration finale pour l'entra√Ænement.
    
    Attributes:
        scalers (Dict): Dictionnaire des scalers ajust√©s
        feature_names (List[str]): Liste des noms de features finales
        is_fitted (bool): Indicateur si le preprocessor a √©t√© ajust√©
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le preprocessor avec la configuration.
        
        Args:
            config (Dict, optional): Configuration de preprocessing.
                                   Si None, utilise la configuration par d√©faut.
        """
        self.config = config or self._get_default_config()
        self.scalers = {}
        self.feature_names = []
        self.is_fitted = False
        self.logger = setup_logging("INFO")
        
    def _get_default_config(self) -> Dict:
        """Configuration par d√©faut du preprocessing."""
        return {
            "temporal_features": True,
            "amount_features": True, 
            "pca_features": True,
            "scaling_method": "standard",
            "remove_outliers": True,
            "outlier_threshold": 3.0
        }
    
    def fit(self, df: pd.DataFrame) -> 'DataPreprocessor':
        """
        Ajuste le preprocessor sur les donn√©es d'entra√Ænement.
        
        Args:
            df (pd.DataFrame): Donn√©es d'entra√Ænement
            
        Returns:
            DataPreprocessor: Self pour cha√Ænage
            
        Raises:
            ValueError: Si les donn√©es ne sont pas valides
        """
        logger.info("D√©but de l'ajustement du preprocessor")
        
        # Validation des donn√©es
        required_cols = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]
        validation = validate_dataset_quality(df, required_cols)
        
        if not validation['is_valid']:
            raise ValueError(f"Donn√©es invalides: {validation['message']}")
        
        # √âtapes de preprocessing
        df_processed = df.copy()
        df_processed = self._clean_data(df_processed)
        df_processed = self._create_features(df_processed)
        df_processed = self._fit_scalers(df_processed)
        
        self.is_fitted = True
        self.feature_names = [col for col in df_processed.columns if col != 'Class']
        
        logger.info(f"Preprocessor ajust√© avec {len(self.feature_names)} features")
        return self
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Applique le preprocessing sur de nouvelles donn√©es.
        
        Args:
            df (pd.DataFrame): Donn√©es √† transformer
            
        Returns:
            pd.DataFrame: Donn√©es transform√©es
            
        Raises:
            ValueError: Si le preprocessor n'a pas √©t√© ajust√©
        """
        if not self.is_fitted:
            raise ValueError("Le preprocessor doit √™tre ajust√© avant transformation")
        
        logger.info("Transformation des donn√©es")
        
        df_processed = df.copy()
        df_processed = self._clean_data(df_processed)
        df_processed = self._create_features(df_processed)
        df_processed = self._apply_scaling(df_processed)
        
        # Assurer la coh√©rence des colonnes
        for col in self.feature_names:
            if col not in df_processed.columns:
                df_processed[col] = 0  # Valeur par d√©faut
        
        return df_processed[self.feature_names + (['Class'] if 'Class' in df_processed.columns else [])]
    
    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajuste et transforme les donn√©es en une seule √©tape.
        
        Args:
            df (pd.DataFrame): Donn√©es √† ajuster et transformer
            
        Returns:
            pd.DataFrame: Donn√©es transform√©es
        """
        return self.fit(df).transform(df)
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoie les donn√©es (doublons, valeurs manquantes, outliers)."""
        logger.debug("Nettoyage des donn√©es")
        
        # Supprimer les doublons
        initial_rows = len(df)
        df = df.drop_duplicates()
        dropped_duplicates = initial_rows - len(df)
        
        if dropped_duplicates > 0:
            logger.info(f"Supprim√©s {dropped_duplicates} doublons")
        
        # G√©rer les valeurs manquantes (tr√®s rares dans ce dataset)
        if df.isnull().any().any():
            logger.warning("Valeurs manquantes d√©tect√©es, remplacement par m√©diane")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        
        # Supprimer les outliers extr√™mes sur Amount
        if self.config.get("remove_outliers", True):
            threshold = self.config.get("outlier_threshold", 3.0)
            z_scores = np.abs((df['Amount'] - df['Amount'].mean()) / df['Amount'].std())
            outliers = len(df[z_scores > threshold])
            df = df[z_scores <= threshold]
            
            if outliers > 0:
                logger.info(f"Supprim√©s {outliers} outliers extr√™mes")
        
        return df
    
    def _create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e les nouvelles features d'ing√©nierie."""
        logger.debug("Cr√©ation des features")
        
        df = df.copy()
        
        # Features temporelles
        if self.config.get("temporal_features", True):
            df = self._create_temporal_features(df)
        
        # Features de montant
        if self.config.get("amount_features", True):
            df = self._create_amount_features(df)
        
        # Features PCA enrichies
        if self.config.get("pca_features", True):
            df = self._create_pca_features(df)
        
        return df
    
    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e les features temporelles."""
        # Convertir Time en heures (Time est en secondes depuis d√©but)
        df['Hour'] = (df['Time'] / 3600) % 24  # Heure du jour (0-23)
        df['Day'] = (df['Time'] / 86400).astype(int)  # Jour depuis d√©but
        
        # Indicateurs binaires
        df['Is_Night'] = ((df['Hour'] >= 22) | (df['Hour'] < 6)).astype(int)
        df['Is_Weekend'] = (df['Day'] % 7 >= 5).astype(int)  # Simule weekend
        
        return df
    
    def _create_amount_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e les features bas√©es sur le montant."""
        # Log du montant (+ 1 pour √©viter log(0))
        df['Amount_log'] = np.log1p(df['Amount'])
        
        # Cat√©gories de montant
        df['Amount_Category'] = pd.cut(
            df['Amount'], 
            bins=[0, 10, 50, 200, 1000, float('inf')],
            labels=['Micro', 'Small', 'Medium', 'Large', 'XLarge']
        ).cat.codes
        
        # Z-score du montant par jour
        daily_stats = df.groupby('Day')['Amount'].agg(['mean', 'std'])
        df = df.merge(daily_stats, on='Day', suffixes=('', '_day'))
        df['Amount_Daily_Zscore'] = (df['Amount'] - df['Amount_day']) / (df['Amount_day'].std() + 1e-8)
        
        # Nettoyer les colonnes temporaires
        df = df.drop(['Amount_day', 'Amount_day'], axis=1, errors='ignore')
        
        return df
    
    def _create_pca_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cr√©e des features enrichies bas√©es sur les composantes PCA."""
        pca_cols = [f'V{i}' for i in range(1, 29)]
        
        # Magnitude euclidienne des composantes PCA
        df['PCA_Magnitude'] = np.sqrt((df[pca_cols] ** 2).sum(axis=1))
        
        # Nombre de composantes "extr√™mes" (> 2 √©carts-type)
        pca_extreme = (np.abs(df[pca_cols]) > 2).sum(axis=1)
        df['PCA_Extreme_Count'] = pca_extreme
        
        return df
    
    def _fit_scalers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ajuste les scalers sur les donn√©es."""
        logger.debug("Ajustement des scalers")
        
        # D√©finir les groupes de colonnes pour diff√©rents types de scaling
        scaling_groups = {
            'standard': ['Time', 'Hour', 'Amount_Daily_Zscore', 'PCA_Magnitude'],
            'robust': ['Amount'],
            'minmax': ['Amount_log', 'Amount_Category', 'PCA_Extreme_Count']
        }
        
        scaler_classes = {
            'standard': StandardScaler,
            'robust': RobustScaler, 
            'minmax': MinMaxScaler
        }
        
        for scaler_type, columns in scaling_groups.items():
            # Filtrer les colonnes qui existent r√©ellement
            existing_cols = [col for col in columns if col in df.columns]
            
            if existing_cols:
                scaler = scaler_classes[scaler_type]()
                scaler.fit(df[existing_cols])
                self.scalers[scaler_type] = {
                    'scaler': scaler,
                    'columns': existing_cols
                }
        
        return df
    
    def _apply_scaling(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique les scalers ajust√©s."""
        df = df.copy()
        
        for scaler_type, scaler_info in self.scalers.items():
            scaler = scaler_info['scaler']
            columns = scaler_info['columns']
            
            # V√©rifier que toutes les colonnes existent
            existing_cols = [col for col in columns if col in df.columns]
            if existing_cols:
                df[existing_cols] = scaler.transform(df[existing_cols])
        
        return df


def split_data(
    df: pd.DataFrame,
    target_column: str = 'Class',
    test_size: float = 0.2,
    val_size: float = 0.2,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Divise les donn√©es en ensembles train/validation/test stratifi√©s.
    
    Args:
        df (pd.DataFrame): Donn√©es compl√®tes
        target_column (str): Nom de la colonne cible
        test_size (float): Proportion pour le test set
        val_size (float): Proportion pour le validation set (sur les donn√©es restantes)
        random_state (int): Seed pour reproductibilit√©
        
    Returns:
        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Train, validation, test sets
        
    Examples:
        >>> train_df, val_df, test_df = split_data(df)
        >>> print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
    """
    # Premier split: train+val vs test
    train_val, test = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df[target_column]
    )
    
    # Deuxi√®me split: train vs validation
    train, val = train_test_split(
        train_val,
        test_size=val_size,
        random_state=random_state,
        stratify=train_val[target_column]
    )
    
    logger.info(f"Donn√©es divis√©es - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")
    
    return train, val, test


def get_feature_importance_summary(
    feature_names: List[str],
    feature_importance: np.ndarray,
    top_n: int = 10
) -> pd.DataFrame:
    """
    Cr√©e un r√©sum√© des features les plus importantes.
    
    Args:
        feature_names (List[str]): Noms des features
        feature_importance (np.ndarray): Importances des features
        top_n (int): Nombre de top features √† retourner
        
    Returns:
        pd.DataFrame: DataFrame avec features et importances tri√©es
    """
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    })
    
    return importance_df.nlargest(top_n, 'importance').reset_index(drop=True)


if __name__ == "__main__":
    # Test du preprocessor avec donn√©es factices
    logger.info("Test du module data_processing")
    
    # Cr√©er des donn√©es de test
    np.random.seed(42)
    n_samples = 1000
    
    test_data = {
        'Time': np.random.uniform(0, 172800, n_samples),  # 2 jours en secondes
        'Amount': np.random.lognormal(2, 1, n_samples),  # Distribution r√©aliste
        'Class': np.random.choice([0, 1], n_samples, p=[0.998, 0.002])  # D√©s√©quilibr√©
    }
    
    # Ajouter les features V1-V28 (PCA fictives)
    for i in range(1, 29):
        test_data[f'V{i}'] = np.random.normal(0, 1, n_samples)
    
    test_df = pd.DataFrame(test_data)
    
    # Tester le preprocessor
    preprocessor = DataPreprocessor()
    
    try:
        processed_df = preprocessor.fit_transform(test_df)
        logger.info(f"‚úÖ Preprocessing r√©ussi: {processed_df.shape}")
        logger.info(f"‚úÖ Features cr√©√©es: {len(preprocessor.feature_names)}")
        
        # Tester le split
        train_df, val_df, test_df = split_data(processed_df)
        logger.info("‚úÖ Split des donn√©es r√©ussi")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du test: {str(e)}")
        raise
    
    logger.info("üéâ Tests du module data_processing termin√©s avec succ√®s!")