"""
Module d'Entra√Ænement des Mod√®les ML pour D√©tection de Fraude
=============================================================

Ce module contient toutes les fonctions pour l'entra√Ænement, l'optimisation
et l'√©valuation des mod√®les de machine learning pour la d√©tection de fraude.

Fonctionnalit√©s principales:
- Entra√Ænement de multiples algorithmes ML
- Optimisation des hyperparam√®tres 
- √âvaluation et comparaison des mod√®les
- Sauvegarde et chargement des mod√®les

Auteur: √âquipe Data Science - N√©obanque Fluzz  
Version: 1.0
Date: Ao√ªt 2025
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    precision_recall_curve, f1_score, precision_score, recall_score
)
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from .utils import setup_logging, save_model_artifacts

# Configuration du logger
logger = setup_logging("INFO")


class FraudModelTrainer:
    """
    Classe principale pour l'entra√Ænement des mod√®les de d√©tection de fraude.
    
    Cette classe encapsule l'entra√Ænement de multiples algorithmes,
    l'optimisation des hyperparam√®tres, et l'√©valuation comparative.
    
    Attributes:
        models (Dict): Dictionnaire des mod√®les disponibles
        best_model: Meilleur mod√®le apr√®s comparaison
        results (Dict): R√©sultats d'√©valuation de tous les mod√®les
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le trainer avec la configuration des mod√®les.
        
        Args:
            config (Dict, optional): Configuration des mod√®les et hyperparam√®tres.
                                   Si None, utilise la configuration par d√©faut.
        """
        self.config = config or self._get_default_config()
        self.models = {}
        self.best_model = None
        self.results = {}
        self.logger = setup_logging("INFO")
        
    def _get_default_config(self) -> Dict:
        """Configuration par d√©faut des mod√®les."""
        return {
            'models': {
                'logistic_regression': {
                    'class': LogisticRegression,
                    'params': {
                        'C': [0.01, 0.1, 1, 10],
                        'penalty': ['l1', 'l2'],
                        'solver': ['liblinear'],
                        'max_iter': [1000]
                    },
                    'enabled': True
                },
                'random_forest': {
                    'class': RandomForestClassifier,
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'max_depth': [5, 10, None],
                        'min_samples_split': [2, 5],
                        'min_samples_leaf': [1, 2],
                        'class_weight': ['balanced']
                    },
                    'enabled': True
                },
                'gradient_boosting': {
                    'class': GradientBoostingClassifier,
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'max_depth': [3, 5, 7],
                        'subsample': [0.8, 1.0]
                    },
                    'enabled': True
                }
            },
            'cv_folds': 5,
            'scoring': 'f1',
            'n_jobs': -1
        }
    
    def train_all_models(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None
    ) -> Dict[str, Any]:
        """
        Entra√Æne tous les mod√®les configur√©s et les compare.
        
        Args:
            X_train (pd.DataFrame): Features d'entra√Ænement
            y_train (pd.Series): Target d'entra√Ænement  
            X_val (pd.DataFrame, optional): Features de validation
            y_val (pd.Series, optional): Target de validation
            
        Returns:
            Dict[str, Any]: R√©sultats de comparaison des mod√®les
        """
        logger.info("D√©but de l'entra√Ænement de tous les mod√®les")
        
        results = {}
        
        for model_name, model_config in self.config['models'].items():
            if not model_config.get('enabled', True):
                logger.info(f"Mod√®le {model_name} d√©sactiv√©, passage au suivant")
                continue
                
            logger.info(f"Entra√Ænement du mod√®le: {model_name}")
            
            try:
                # Entra√Æner le mod√®le avec optimisation hyperparam√®tres
                best_model, cv_results = self._train_single_model(
                    model_name, model_config, X_train, y_train
                )
                
                # √âvaluer sur validation si fournie
                val_scores = {}
                if X_val is not None and y_val is not None:
                    val_scores = self._evaluate_model(best_model, X_val, y_val)
                
                results[model_name] = {
                    'model': best_model,
                    'cv_scores': cv_results,
                    'validation_scores': val_scores,
                    'best_params': best_model.best_params_ if hasattr(best_model, 'best_params_') else {}
                }
                
                logger.info(f"‚úÖ {model_name} termin√© - F1: {cv_results.get('f1', 'N/A'):.3f}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de l'entra√Ænement de {model_name}: {str(e)}")
                continue
        
        self.results = results
        
        # S√©lectionner le meilleur mod√®le
        self.best_model = self._select_best_model(results)
        
        logger.info(f"üèÜ Meilleur mod√®le: {self.best_model['name']} (F1: {self.best_model['score']:.3f})")
        
        return results
    
    def _train_single_model(
        self,
        model_name: str,
        model_config: Dict,
        X_train: pd.DataFrame,
        y_train: pd.Series
    ) -> Tuple[Any, Dict]:
        """Entra√Æne un seul mod√®le avec optimisation des hyperparam√®tres."""
        
        # Initialiser le mod√®le
        model_class = model_config['class']
        base_model = model_class(random_state=42)
        
        # Configuration de la validation crois√©e
        cv = StratifiedKFold(
            n_splits=self.config['cv_folds'],
            shuffle=True,
            random_state=42
        )
        
        # Grid Search pour optimisation hyperparam√®tres
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=model_config['params'],
            cv=cv,
            scoring=self.config['scoring'],
            n_jobs=self.config['n_jobs'],
            verbose=0
        )
        
        # Entra√Ænement
        grid_search.fit(X_train, y_train)
        
        # √âvaluation cross-validation du meilleur mod√®le
        cv_scores = cross_val_score(
            grid_search.best_estimator_,
            X_train,
            y_train,
            cv=cv,
            scoring=self.config['scoring']
        )
        
        cv_results = {
            'f1': cv_scores.mean(),
            'f1_std': cv_scores.std(),
            'scores_detail': cv_scores
        }
        
        return grid_search, cv_results
    
    def _evaluate_model(self, model: Any, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """√âvalue un mod√®le sur un ensemble de test."""
        
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # Calcul des m√©triques
        metrics = {
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()
        }
        
        return metrics
    
    def _select_best_model(self, results: Dict) -> Dict:
        """S√©lectionne le meilleur mod√®le bas√© sur le F1-score de validation crois√©e."""
        
        best_score = -1
        best_model_name = None
        
        for model_name, model_results in results.items():
            f1_score = model_results['cv_scores']['f1']
            
            if f1_score > best_score:
                best_score = f1_score
                best_model_name = model_name
        
        if best_model_name:
            return {
                'name': best_model_name,
                'model': results[best_model_name]['model'],
                'score': best_score
            }
        
        return None
    
    def generate_model_comparison_report(self) -> str:
        """G√©n√®re un rapport de comparaison des mod√®les."""
        
        if not self.results:
            return "Aucun mod√®le entra√Æn√© pour le moment."
        
        report_lines = [
            "# Rapport de Comparaison des Mod√®les ML",
            f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## R√©sultats Cross-Validation",
            "",
            "| Mod√®le | F1-Score | Std | Meilleur |",
            "|--------|----------|-----|----------|"
        ]
        
        for model_name, model_results in self.results.items():
            cv_results = model_results['cv_scores']
            f1_mean = cv_results['f1']
            f1_std = cv_results['f1_std']
            is_best = "üèÜ" if (self.best_model and self.best_model['name'] == model_name) else ""
            
            report_lines.append(f"| {model_name} | {f1_mean:.3f} | {f1_std:.3f} | {is_best} |")
        
        # Ajouter les param√®tres du meilleur mod√®le
        if self.best_model:
            report_lines.extend([
                "",
                "## Meilleur Mod√®le - Param√®tres Optimaux",
                "",
                f"**Mod√®le:** {self.best_model['name']}",
                f"**Score F1:** {self.best_model['score']:.3f}",
                "",
                "**Hyperparam√®tres:**"
            ])
            
            model_results = self.results[self.best_model['name']]
            best_params = model_results['best_params']
            
            for param, value in best_params.items():
                report_lines.append(f"- **{param}:** {value}")
        
        return "\n".join(report_lines)
    
    def save_best_model(self, output_dir: str, model_name: Optional[str] = None) -> Dict[str, str]:
        """
        Sauvegarde le meilleur mod√®le et ses m√©tadonn√©es.
        
        Args:
            output_dir (str): R√©pertoire de destination
            model_name (str, optional): Nom personnalis√© pour les fichiers
            
        Returns:
            Dict[str, str]: Chemins des fichiers sauvegard√©s
        """
        if not self.best_model:
            raise ValueError("Aucun mod√®le n'a √©t√© entra√Æn√©")
        
        # Nom par d√©faut
        if not model_name:
            model_name = f"fraud_model_{self.best_model['name']}"
        
        # Pr√©parer les m√©tadonn√©es
        metadata = {
            'model_type': self.best_model['name'],
            'f1_score': self.best_model['score'],
            'training_date': datetime.now().isoformat(),
            'best_params': self.results[self.best_model['name']]['best_params'],
            'cv_results': self.results[self.best_model['name']]['cv_scores']
        }
        
        # Artefacts √† sauvegarder
        artifacts = {
            'model': self.best_model['model'],
            'metadata': metadata
        }
        
        # Sauvegarder
        saved_paths = save_model_artifacts(artifacts, output_dir, model_name)
        
        logger.info(f"‚úÖ Mod√®le sauvegard√© dans {output_dir}")
        return saved_paths


def compare_models_performance(
    models_results: Dict[str, Any],
    X_test: pd.DataFrame,
    y_test: pd.Series
) -> pd.DataFrame:
    """
    Compare les performances de plusieurs mod√®les sur un ensemble de test.
    
    Args:
        models_results (Dict[str, Any]): R√©sultats des mod√®les entra√Æn√©s
        X_test (pd.DataFrame): Features de test
        y_test (pd.Series): Target de test
        
    Returns:
        pd.DataFrame: Tableau comparatif des performances
    """
    comparison_data = []
    
    for model_name, model_info in models_results.items():
        model = model_info['model']
        
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # M√©triques
        metrics = {
            'Model': model_name,
            'Precision': precision_score(y_test, y_pred),
            'Recall': recall_score(y_test, y_pred), 
            'F1-Score': f1_score(y_test, y_pred),
            'ROC-AUC': roc_auc_score(y_test, y_pred_proba)
        }
        
        comparison_data.append(metrics)
    
    comparison_df = pd.DataFrame(comparison_data)
    return comparison_df.sort_values('F1-Score', ascending=False)


def calculate_business_metrics(
    y_true: pd.Series,
    y_pred: pd.Series,
    y_pred_proba: pd.Series,
    fraud_cost: float = 100.0,
    investigation_cost: float = 10.0
) -> Dict[str, float]:
    """
    Calcule les m√©triques business pour l'√©valuation du mod√®le.
    
    Args:
        y_true (pd.Series): Vraies labels
        y_pred (pd.Series): Pr√©dictions
        y_pred_proba (pd.Series): Probabilit√©s de fraude
        fraud_cost (float): Co√ªt d'une fraude non d√©tect√©e
        investigation_cost (float): Co√ªt d'investigation d'une alerte
        
    Returns:
        Dict[str, float]: M√©triques business
    """
    # Matrice de confusion
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    # Co√ªts
    cost_missed_frauds = fn * fraud_cost  # Fraudes rat√©es
    cost_investigations = fp * investigation_cost  # Fausses alertes
    total_cost = cost_missed_frauds + cost_investigations
    
    # B√©n√©fices
    prevented_frauds = tp * fraud_cost
    net_benefit = prevented_frauds - total_cost
    
    # ROI
    investigation_total_cost = (tp + fp) * investigation_cost
    roi = (prevented_frauds - investigation_total_cost) / max(investigation_total_cost, 1)
    
    return {
        'total_cost': total_cost,
        'cost_missed_frauds': cost_missed_frauds,
        'cost_false_alerts': cost_investigations,
        'prevented_fraud_value': prevented_frauds,
        'net_benefit': net_benefit,
        'roi': roi,
        'cost_per_transaction': total_cost / len(y_true)
    }


if __name__ == "__main__":
    # Test du module avec donn√©es factices
    logger.info("Test du module model_training")
    
    from sklearn.datasets import make_classification
    
    # Cr√©er des donn√©es de test d√©s√©quilibr√©es
    X, y = make_classification(
        n_samples=5000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        weights=[0.995, 0.005],  # Tr√®s d√©s√©quilibr√©
        random_state=42
    )
    
    X_train = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
    y_train = pd.Series(y)
    
    # Tester le trainer
    trainer = FraudModelTrainer()
    
    try:
        # Entra√Æner les mod√®les (configuration r√©duite pour test rapide)
        test_config = {
            'models': {
                'logistic_regression': {
                    'class': LogisticRegression,
                    'params': {'C': [0.1, 1], 'max_iter': [1000]},
                    'enabled': True
                },
                'random_forest': {
                    'class': RandomForestClassifier,
                    'params': {'n_estimators': [50], 'max_depth': [5]},
                    'enabled': True
                }
            },
            'cv_folds': 3,
            'scoring': 'f1',
            'n_jobs': 1
        }
        
        trainer.config = test_config
        results = trainer.train_all_models(X_train, y_train)
        
        logger.info(f"‚úÖ {len(results)} mod√®les entra√Æn√©s")
        logger.info(f"‚úÖ Meilleur mod√®le: {trainer.best_model['name'] if trainer.best_model else 'Aucun'}")
        
        # G√©n√©rer rapport
        report = trainer.generate_model_comparison_report()
        logger.info("‚úÖ Rapport g√©n√©r√©")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du test: {str(e)}")
        raise
    
    logger.info("üéâ Tests du module model_training termin√©s avec succ√®s!")