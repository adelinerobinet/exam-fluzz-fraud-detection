"""
Module de G√©n√©ration de Donn√©es Synth√©tiques pour D√©tection de Fraude
======================================================================

Ce module contient toutes les fonctions pour g√©n√©rer des donn√©es synth√©tiques
afin de r√©√©quilibrer le dataset de fraude bancaire extr√™mement d√©s√©quilibr√©.

Fonctionnalit√©s principales:
- G√©n√©ration SMOTE et variantes
- G√©n√©ration SDV (Synthetic Data Vault)
- √âvaluation de la qualit√© des donn√©es synth√©tiques  
- S√©lection de la meilleure m√©thode

Auteur: √âquipe Data Science - N√©obanque Fluzz
Version: 1.0
Date: Ao√ªt 2025
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Import conditionnel de SDV (peut ne pas √™tre install√©)
try:
    from sdv.single_table import GaussianCopulaSynthesizer
    from sdv.metadata import SingleTableMetadata
    SDV_AVAILABLE = True
except ImportError:
    SDV_AVAILABLE = False

from .utils import setup_logging, calculate_class_balance_metrics

# Configuration du logger
logger = setup_logging("INFO")


class SyntheticDataGenerator:
    """
    Classe principale pour la g√©n√©ration de donn√©es synth√©tiques.
    
    Cette classe encapsule diff√©rentes m√©thodes de g√©n√©ration de donn√©es
    synth√©tiques et permet de comparer leur qualit√© pour choisir la meilleure.
    
    Attributes:
        methods (Dict): Dictionnaire des m√©thodes disponibles
        generated_data (Dict): Donn√©es g√©n√©r√©es par chaque m√©thode
        quality_scores (Dict): Scores de qualit√© pour chaque m√©thode
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialise le g√©n√©rateur avec la configuration.
        
        Args:
            config (Dict, optional): Configuration des m√©thodes de g√©n√©ration.
                                   Si None, utilise la configuration par d√©faut.
        """
        self.config = config or self._get_default_config()
        self.methods = {}
        self.generated_data = {}
        self.quality_scores = {}
        self.logger = setup_logging("INFO")
        
    def _get_default_config(self) -> Dict:
        """Configuration par d√©faut des m√©thodes de g√©n√©ration."""
        return {
            'target_samples': 2000,
            'methods': {
                'smote': {
                    'enabled': True,
                    'params': {
                        'sampling_strategy': 0.1,  # Ratio final minority/majority
                        'k_neighbors': 5,
                        'random_state': 42
                    }
                },
                'borderline_smote': {
                    'enabled': True,
                    'params': {
                        'sampling_strategy': 0.1,
                        'k_neighbors': 5,
                        'kind': 'borderline-1',
                        'random_state': 42
                    }
                },
                'sdv': {
                    'enabled': SDV_AVAILABLE,
                    'params': {
                        'num_samples': 2000,
                        'enforce_min_max_values': True,
                        'enforce_rounding': False
                    }
                }
            },
            'evaluation': {
                'test_size': 0.3,
                'discriminator_model': RandomForestClassifier,
                'quality_threshold': 0.8
            }
        }
    
    def generate_all_methods(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        target_column: str = 'Class'
    ) -> Dict[str, pd.DataFrame]:
        """
        G√©n√®re des donn√©es synth√©tiques avec toutes les m√©thodes configur√©es.
        
        Args:
            X (pd.DataFrame): Features originales
            y (pd.Series): Target originale
            target_column (str): Nom de la colonne target
            
        Returns:
            Dict[str, pd.DataFrame]: Donn√©es g√©n√©r√©es par chaque m√©thode
        """
        logger.info("D√©but de la g√©n√©ration de donn√©es synth√©tiques")
        
        results = {}
        
        # Analyser le d√©s√©quilibre initial
        balance_metrics = calculate_class_balance_metrics(y)
        logger.info(f"D√©s√©quilibre initial: {balance_metrics['imbalance_ratio']:.1f}:1")
        
        for method_name, method_config in self.config['methods'].items():
            if not method_config.get('enabled', False):
                logger.info(f"M√©thode {method_name} d√©sactiv√©e")
                continue
                
            logger.info(f"G√©n√©ration avec {method_name}...")
            
            try:
                synthetic_df = self._generate_single_method(
                    method_name, method_config, X, y, target_column
                )
                
                if synthetic_df is not None:
                    results[method_name] = synthetic_df
                    
                    # Calculer le nouveau ratio
                    new_balance = calculate_class_balance_metrics(synthetic_df[target_column])
                    logger.info(f"‚úÖ {method_name}: {len(synthetic_df)} √©chantillons, "
                              f"ratio {new_balance['imbalance_ratio']:.1f}:1")
                else:
                    logger.warning(f"‚ö†Ô∏è {method_name}: G√©n√©ration √©chou√©e")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la g√©n√©ration avec {method_name}: {str(e)}")
                continue
        
        self.generated_data = results
        return results
    
    def _generate_single_method(
        self,
        method_name: str,
        method_config: Dict,
        X: pd.DataFrame,
        y: pd.Series,
        target_column: str
    ) -> Optional[pd.DataFrame]:
        """G√©n√®re des donn√©es avec une m√©thode sp√©cifique."""
        
        if method_name in ['smote', 'borderline_smote']:
            return self._generate_smote_variant(method_name, method_config, X, y, target_column)
        elif method_name == 'sdv':
            return self._generate_sdv(method_config, X, y, target_column)
        else:
            logger.error(f"M√©thode inconnue: {method_name}")
            return None
    
    def _generate_smote_variant(
        self,
        method_name: str,
        method_config: Dict,
        X: pd.DataFrame,
        y: pd.Series,
        target_column: str
    ) -> pd.DataFrame:
        """G√©n√®re des donn√©es avec les variantes SMOTE."""
        
        # S√©lectionner la classe SMOTE appropri√©e
        smote_classes = {
            'smote': SMOTE,
            'borderline_smote': BorderlineSMOTE,
            'svm_smote': SVMSMOTE
        }
        
        smote_class = smote_classes.get(method_name, SMOTE)
        
        # Cr√©er et configurer SMOTE
        smote = smote_class(**method_config['params'])
        
        # G√©n√©ration
        X_resampled, y_resampled = smote.fit_resample(X, y)
        
        # Cr√©er le DataFrame r√©sultat
        result_df = X_resampled.copy()
        result_df[target_column] = y_resampled
        
        return result_df
    
    def _generate_sdv(
        self,
        method_config: Dict,
        X: pd.DataFrame,
        y: pd.Series,
        target_column: str
    ) -> Optional[pd.DataFrame]:
        """G√©n√®re des donn√©es avec SDV (Synthetic Data Vault)."""
        
        if not SDV_AVAILABLE:
            logger.warning("SDV n'est pas disponible, installation requise")
            return None
        
        try:
            # Pr√©parer les donn√©es compl√®tes
            full_data = X.copy()
            full_data[target_column] = y
            
            # Ne garder que les fraudes pour l'entra√Ænement SDV
            fraud_data = full_data[full_data[target_column] == 1].copy()
            
            if len(fraud_data) < 10:
                logger.warning("Pas assez de fraudes pour entra√Æner SDV")
                return None
            
            # Cr√©er les m√©tadonn√©es
            metadata = SingleTableMetadata()
            metadata.detect_from_dataframe(fraud_data)
            
            # Configurer le synthesizer
            synthesizer = GaussianCopulaSynthesizer(
                metadata,
                enforce_min_max_values=method_config['params'].get('enforce_min_max_values', True),
                enforce_rounding=method_config['params'].get('enforce_rounding', False)
            )
            
            # Entra√Æner
            synthesizer.fit(fraud_data)
            
            # G√©n√©rer des √©chantillons synth√©tiques
            num_samples = method_config['params'].get('num_samples', 1000)
            synthetic_frauds = synthesizer.sample(num_samples)
            
            # S'assurer que la colonne Class est bien d√©finie
            synthetic_frauds[target_column] = 1
            
            # Combiner avec les donn√©es originales
            normal_data = full_data[full_data[target_column] == 0].copy()
            result_df = pd.concat([normal_data, synthetic_frauds], ignore_index=True)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Erreur SDV: {str(e)}")
            return None
    
    def evaluate_quality(
        self,
        original_data: pd.DataFrame,
        target_column: str = 'Class'
    ) -> Dict[str, float]:
        """
        √âvalue la qualit√© des donn√©es synth√©tiques g√©n√©r√©es.
        
        Args:
            original_data (pd.DataFrame): Donn√©es originales pour comparaison
            target_column (str): Nom de la colonne target
            
        Returns:
            Dict[str, float]: Scores de qualit√© pour chaque m√©thode
        """
        logger.info("√âvaluation de la qualit√© des donn√©es synth√©tiques")
        
        quality_scores = {}
        
        for method_name, synthetic_data in self.generated_data.items():
            try:
                score = self._evaluate_single_method(
                    method_name, synthetic_data, original_data, target_column
                )
                quality_scores[method_name] = score
                logger.info(f"‚úÖ {method_name}: Score qualit√© {score:.3f}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur √©valuation {method_name}: {str(e)}")
                quality_scores[method_name] = 0.0
        
        self.quality_scores = quality_scores
        return quality_scores
    
    def _evaluate_single_method(
        self,
        method_name: str,
        synthetic_data: pd.DataFrame,
        original_data: pd.DataFrame,
        target_column: str
    ) -> float:
        """√âvalue la qualit√© d'une m√©thode de g√©n√©ration sp√©cifique."""
        
        # Pr√©parer les donn√©es pour l'√©valuation discriminateur
        X_orig = original_data.drop(columns=[target_column])
        X_synth = synthetic_data.drop(columns=[target_column])
        
        # Cr√©er les labels (0=original, 1=synth√©tique)
        y_orig = np.zeros(len(X_orig))
        y_synth = np.ones(len(X_synth))
        
        # Combiner les donn√©es
        X_combined = pd.concat([X_orig, X_synth], ignore_index=True)
        y_combined = np.concatenate([y_orig, y_synth])
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_combined, y_combined,
            test_size=self.config['evaluation']['test_size'],
            random_state=42,
            stratify=y_combined
        )
        
        # Normaliser les donn√©es
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Entra√Æner discriminateur
        discriminator = RandomForestClassifier(n_estimators=100, random_state=42)
        discriminator.fit(X_train_scaled, y_train)
        
        # Pr√©dire
        y_pred = discriminator.predict(X_test_scaled)
        
        # Score de qualit√©: 1 - accuracy du discriminateur
        # Si le discriminateur ne peut pas distinguer, la qualit√© est haute
        discriminator_accuracy = accuracy_score(y_test, y_pred)
        quality_score = 1 - discriminator_accuracy
        
        return quality_score
    
    def select_best_method(self) -> Tuple[str, pd.DataFrame]:
        """
        S√©lectionne la meilleure m√©thode bas√©e sur les scores de qualit√©.
        
        Returns:
            Tuple[str, pd.DataFrame]: Nom de la meilleure m√©thode et ses donn√©es
            
        Raises:
            ValueError: Si aucune m√©thode n'a √©t√© √©valu√©e
        """
        if not self.quality_scores:
            raise ValueError("Aucune m√©thode n'a √©t√© √©valu√©e")
        
        best_method = max(self.quality_scores, key=self.quality_scores.get)
        best_score = self.quality_scores[best_method]
        
        logger.info(f"üèÜ Meilleure m√©thode: {best_method} (score: {best_score:.3f})")
        
        return best_method, self.generated_data[best_method]
    
    def generate_quality_report(self) -> str:
        """G√©n√®re un rapport de qualit√© des donn√©es synth√©tiques."""
        
        if not self.quality_scores:
            return "Aucune √©valuation de qualit√© disponible."
        
        report_lines = [
            "# Rapport de Qualit√© - Donn√©es Synth√©tiques",
            "",
            "## Scores de Qualit√© par M√©thode",
            "",
            "| M√©thode | Score Qualit√© | Statut |",
            "|---------|---------------|--------|"
        ]
        
        threshold = self.config['evaluation']['quality_threshold']
        
        for method, score in sorted(self.quality_scores.items(), key=lambda x: x[1], reverse=True):
            status = "‚úÖ Excellent" if score >= threshold else "‚ö†Ô∏è Acceptable" if score >= 0.5 else "‚ùå Faible"
            report_lines.append(f"| {method} | {score:.3f} | {status} |")
        
        # Recommandations
        report_lines.extend([
            "",
            "## Recommandations",
            ""
        ])
        
        best_method = max(self.quality_scores, key=self.quality_scores.get)
        best_score = self.quality_scores[best_method]
        
        if best_score >= threshold:
            report_lines.append(f"‚úÖ **Recommand√©**: Utiliser `{best_method}` (score: {best_score:.3f})")
        else:
            report_lines.append(f"‚ö†Ô∏è **Attention**: Meilleur score ({best_score:.3f}) sous le seuil ({threshold})")
            report_lines.append("- Consid√©rer ajuster les param√®tres de g√©n√©ration")
            report_lines.append("- V√©rifier la qualit√© des donn√©es originales")
        
        return "\n".join(report_lines)


def create_balanced_dataset(
    X: pd.DataFrame,
    y: pd.Series,
    method: str = 'smote',
    target_ratio: float = 0.1,
    **kwargs
) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Cr√©e un dataset √©quilibr√© avec la m√©thode sp√©cifi√©e.
    
    Args:
        X (pd.DataFrame): Features originales
        y (pd.Series): Target originale
        method (str): M√©thode √† utiliser ('smote', 'borderline_smote', 'sdv')
        target_ratio (float): Ratio final minority/majority souhait√©
        **kwargs: Param√®tres additionnels pour la m√©thode
        
    Returns:
        Tuple[pd.DataFrame, pd.Series]: Features et target √©quilibr√©es
        
    Examples:
        >>> X_balanced, y_balanced = create_balanced_dataset(X, y, 'smote', 0.1)
    """
    config = {
        'methods': {
            method: {
                'enabled': True,
                'params': {
                    'sampling_strategy': target_ratio,
                    'random_state': 42,
                    **kwargs
                }
            }
        }
    }
    
    generator = SyntheticDataGenerator(config)
    
    # Cr√©er un DataFrame temporaire pour la g√©n√©ration
    temp_df = X.copy()
    temp_df['Class'] = y
    
    results = generator.generate_all_methods(X, y, 'Class')
    
    if method in results:
        balanced_df = results[method]
        X_balanced = balanced_df.drop(columns=['Class'])
        y_balanced = balanced_df['Class']
        return X_balanced, y_balanced
    else:
        logger.error(f"√âchec g√©n√©ration avec {method}")
        return X, y


if __name__ == "__main__":
    # Test du module avec donn√©es factices
    logger.info("Test du module synthetic_data")
    
    from sklearn.datasets import make_classification
    
    # Cr√©er des donn√©es de test tr√®s d√©s√©quilibr√©es
    X, y = make_classification(
        n_samples=1000,
        n_features=10,
        n_informative=8,
        n_redundant=2,
        weights=[0.99, 0.01],  # 1% de fraudes
        random_state=42
    )
    
    X_df = pd.DataFrame(X, columns=[f'V{i}' for i in range(1, 11)])
    y_series = pd.Series(y)
    
    # Analyser le d√©s√©quilibre initial
    initial_balance = calculate_class_balance_metrics(y_series)
    logger.info(f"D√©s√©quilibre initial: {initial_balance['imbalance_ratio']:.1f}:1")
    
    # Tester le g√©n√©rateur (sans SDV pour √©viter les d√©pendances)
    test_config = {
        'methods': {
            'smote': {
                'enabled': True,
                'params': {'sampling_strategy': 0.2, 'k_neighbors': 3}
            },
            'borderline_smote': {
                'enabled': True,
                'params': {'sampling_strategy': 0.2, 'k_neighbors': 3}
            }
        }
    }
    
    generator = SyntheticDataGenerator(test_config)
    
    try:
        # G√©n√©rer donn√©es synth√©tiques
        results = generator.generate_all_methods(X_df, y_series, 'Class')
        logger.info(f"‚úÖ {len(results)} m√©thodes ont g√©n√©r√© des donn√©es")
        
        # √âvaluer qualit√©
        original_df = X_df.copy()
        original_df['Class'] = y_series
        
        quality_scores = generator.evaluate_quality(original_df, 'Class')
        logger.info("‚úÖ √âvaluation qualit√© termin√©e")
        
        # S√©lectionner meilleure m√©thode
        if quality_scores:
            best_method, best_data = generator.select_best_method()
            logger.info(f"‚úÖ Meilleure m√©thode s√©lectionn√©e: {best_method}")
        
        # G√©n√©rer rapport
        report = generator.generate_quality_report()
        logger.info("‚úÖ Rapport g√©n√©r√©")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du test: {str(e)}")
        raise
    
    logger.info("üéâ Tests du module synthetic_data termin√©s avec succ√®s!")