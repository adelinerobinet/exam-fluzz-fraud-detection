"""
Utilitaires pour le Pipeline de DÃ©tection de Fraude Bancaire
==============================================================

Ce module contient des fonctions utilitaires rÃ©utilisables pour
le pipeline de machine learning de dÃ©tection de fraude.

Modules principaux:
- Validation des donnÃ©es
- GÃ©nÃ©ration de donnÃ©es synthÃ©tiques
- Logging et reporting
- MÃ©triques de qualitÃ©

Auteur: Ã‰quipe Data Science - NÃ©obanque Fluzz
Version: 1.0
Date: AoÃ»t 2025
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional, Union, List
import logging
from datetime import datetime
import pickle
from pathlib import Path


def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None) -> logging.Logger:
    """
    Configure le systÃ¨me de logging pour le pipeline.
    
    Args:
        log_level (str, optional): Niveau de log ('DEBUG', 'INFO', 'WARNING', 'ERROR').
                                  DÃ©faut: 'INFO'
        log_file (str, optional): Chemin vers le fichier de log. Si None, log sur console.
                                 DÃ©faut: None
    
    Returns:
        logging.Logger: Logger configurÃ©
        
    Examples:
        >>> logger = setup_logging("DEBUG", "logs/pipeline.log")
        >>> logger.info("Pipeline dÃ©marrÃ©")
    """
    logger = logging.getLogger("fraud_detection")
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # Ã‰viter les doublons de handlers
    if logger.handlers:
        logger.handlers.clear()
    
    # Format des messages
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Handler console
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Handler fichier si spÃ©cifiÃ©
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger


def validate_dataset_quality(
    df: pd.DataFrame,
    required_columns: List[str],
    min_rows: int = 1000,
    max_missing_pct: float = 0.05
) -> Dict[str, Union[bool, str, int, float]]:
    """
    Valide la qualitÃ© d'un dataset selon plusieurs critÃ¨res.
    
    Effectue des vÃ©rifications complÃ¨tes sur un DataFrame pour s'assurer
    qu'il respecte les standards de qualitÃ© requis pour l'entraÃ®nement
    des modÃ¨les de dÃ©tection de fraude.
    
    Args:
        df (pd.DataFrame): Dataset Ã  valider
        required_columns (List[str]): Liste des colonnes obligatoires
        min_rows (int, optional): Nombre minimum de lignes requis. DÃ©faut: 1000
        max_missing_pct (float, optional): Pourcentage maximum de valeurs manquantes autorisÃ©.
                                         DÃ©faut: 0.05 (5%)
    
    Returns:
        Dict[str, Union[bool, str, int, float]]: Rapport de validation dÃ©taillÃ© contenant:
            - 'is_valid': bool indiquant si le dataset est valide
            - 'message': str avec message d'erreur ou de succÃ¨s
            - 'rows_count': int nombre de lignes
            - 'columns_count': int nombre de colonnes
            - 'missing_percentage': float pourcentage de valeurs manquantes
            - 'missing_columns': List[str] colonnes manquantes
            
    Raises:
        TypeError: Si df n'est pas un DataFrame pandas
        ValueError: Si required_columns est vide
        
    Examples:
        >>> import pandas as pd
        >>> df = pd.DataFrame({
        ...     'Time': [1, 2, 3, 4, 5],
        ...     'Amount': [10.0, 20.0, None, 40.0, 50.0],
        ...     'Class': [0, 1, 0, 1, 0]
        ... })
        >>> result = validate_dataset_quality(df, ['Time', 'Amount', 'Class'])
        >>> print(result['is_valid'])
        False  # Car 20% de valeurs manquantes > 5%
        
        >>> df_clean = df.dropna()
        >>> result = validate_dataset_quality(df_clean, ['Time', 'Amount', 'Class'])
        >>> print(result['is_valid'])
        True
    """
    if not isinstance(df, pd.DataFrame):
        raise TypeError("Le paramÃ¨tre 'df' doit Ãªtre un DataFrame pandas")
    
    if not required_columns:
        raise ValueError("La liste 'required_columns' ne peut pas Ãªtre vide")
    
    # Initialiser le rapport
    report = {
        'is_valid': True,
        'message': 'Dataset valide',
        'rows_count': len(df),
        'columns_count': len(df.columns),
        'missing_percentage': 0.0,
        'missing_columns': []
    }
    
    # VÃ©rifier si le DataFrame est vide
    if df.empty:
        report.update({
            'is_valid': False,
            'message': 'Dataset vide'
        })
        return report
    
    # VÃ©rifier le nombre minimum de lignes
    if len(df) < min_rows:
        report.update({
            'is_valid': False,
            'message': f'Trop peu de lignes: {len(df)} < {min_rows}'
        })
        return report
    
    # VÃ©rifier les colonnes obligatoires
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        report.update({
            'is_valid': False,
            'message': f'Colonnes manquantes: {missing_columns}',
            'missing_columns': missing_columns
        })
        return report
    
    # Calculer le pourcentage de valeurs manquantes
    total_cells = len(df) * len(df.columns)
    missing_cells = df.isnull().sum().sum()
    missing_percentage = (missing_cells / total_cells) * 100
    
    report['missing_percentage'] = missing_percentage
    
    if missing_percentage > max_missing_pct * 100:
        report.update({
            'is_valid': False,
            'message': f'Trop de valeurs manquantes: {missing_percentage:.2f}% > {max_missing_pct*100}%'
        })
        return report
    
    return report


def calculate_class_balance_metrics(y: pd.Series) -> Dict[str, Union[int, float]]:
    """
    Calcule les mÃ©triques de dÃ©sÃ©quilibre des classes.
    
    Analyse la distribution des classes dans la variable cible et calcule
    diffÃ©rentes mÃ©triques utiles pour Ã©valuer le dÃ©sÃ©quilibre.
    
    Args:
        y (pd.Series): Variable cible contenant les labels de classe
        
    Returns:
        Dict[str, Union[int, float]]: MÃ©triques de dÃ©sÃ©quilibre contenant:
            - 'total_samples': nombre total d'Ã©chantillons
            - 'class_counts': dictionnaire {classe: nombre}
            - 'class_percentages': dictionnaire {classe: pourcentage}
            - 'imbalance_ratio': ratio majoritaire/minoritaire
            - 'minority_class': label de la classe minoritaire
            - 'majority_class': label de la classe majoritaire
            
    Raises:
        ValueError: Si y est vide ou contient moins de 2 classes
        TypeError: Si y n'est pas une Series pandas
        
    Examples:
        >>> import pandas as pd
        >>> y = pd.Series([0, 0, 0, 0, 1, 1])  # 4 normales, 2 fraudes
        >>> metrics = calculate_class_balance_metrics(y)
        >>> print(metrics['imbalance_ratio'])
        2.0
        >>> print(metrics['minority_class'])
        1
    """
    if not isinstance(y, pd.Series):
        raise TypeError("Le paramÃ¨tre 'y' doit Ãªtre une Series pandas")
    
    if y.empty:
        raise ValueError("La Series 'y' ne peut pas Ãªtre vide")
    
    # Compter les classes
    class_counts = y.value_counts().to_dict()
    total_samples = len(y)
    
    if len(class_counts) < 2:
        raise ValueError("Il faut au moins 2 classes diffÃ©rentes")
    
    # Calculer les pourcentages
    class_percentages = {
        cls: (count / total_samples) * 100 
        for cls, count in class_counts.items()
    }
    
    # Identifier les classes majoritaire et minoritaire
    majority_class = max(class_counts, key=class_counts.get)
    minority_class = min(class_counts, key=class_counts.get)
    
    # Calculer le ratio de dÃ©sÃ©quilibre
    imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]
    
    return {
        'total_samples': total_samples,
        'class_counts': class_counts,
        'class_percentages': class_percentages,
        'imbalance_ratio': imbalance_ratio,
        'minority_class': minority_class,
        'majority_class': majority_class
    }


def save_model_artifacts(
    artifacts: Dict[str, any],
    output_dir: Union[str, Path],
    prefix: str = "model"
) -> Dict[str, str]:
    """
    Sauvegarde les artefacts d'un modÃ¨le (modÃ¨le, scalers, mÃ©tadonnÃ©es).
    
    Organise et sauvegarde tous les Ã©lÃ©ments nÃ©cessaires pour
    dÃ©ployer et utiliser un modÃ¨le en production.
    
    Args:
        artifacts (Dict[str, any]): Dictionnaire contenant les artefacts Ã  sauvegarder
                                   ClÃ©s attendues: 'model', 'scaler', 'metadata'
        output_dir (Union[str, Path]): RÃ©pertoire de destination
        prefix (str, optional): PrÃ©fixe pour les noms de fichiers. DÃ©faut: 'model'
        
    Returns:
        Dict[str, str]: Dictionnaire des chemins de fichiers sauvegardÃ©s
        
    Raises:
        OSError: Si impossible de crÃ©er le rÃ©pertoire ou sauvegarder
        ValueError: Si artifacts est vide ou mal formatÃ©
        
    Examples:
        >>> from sklearn.ensemble import RandomForestClassifier
        >>> from sklearn.preprocessing import StandardScaler
        >>> 
        >>> model = RandomForestClassifier()
        >>> scaler = StandardScaler()
        >>> metadata = {'version': '1.0', 'features': ['V1', 'V2']}
        >>> 
        >>> artifacts = {
        ...     'model': model,
        ...     'scaler': scaler,
        ...     'metadata': metadata
        ... }
        >>> 
        >>> paths = save_model_artifacts(artifacts, 'models/', 'fraud_detector')
        >>> print(paths['model'])
        'models/fraud_detector_model.pkl'
    """
    if not artifacts:
        raise ValueError("Le dictionnaire 'artifacts' ne peut pas Ãªtre vide")
    
    # CrÃ©er le rÃ©pertoire de sortie
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    saved_paths = {}
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        for artifact_type, artifact_data in artifacts.items():
            filename = f"{prefix}_{artifact_type}_{timestamp}.pkl"
            file_path = output_path / filename
            
            with open(file_path, 'wb') as f:
                pickle.dump(artifact_data, f)
            
            saved_paths[artifact_type] = str(file_path)
            
        # Sauvegarder aussi un index des fichiers
        index_file = output_path / f"{prefix}_index_{timestamp}.json"
        import json
        with open(index_file, 'w') as f:
            json.dump(saved_paths, f, indent=2)
        
        saved_paths['index'] = str(index_file)
        
        return saved_paths
        
    except Exception as e:
        raise OSError(f"Erreur lors de la sauvegarde: {str(e)}")


def load_model_artifacts(index_file: Union[str, Path]) -> Dict[str, any]:
    """
    Charge les artefacts d'un modÃ¨le depuis un fichier d'index.
    
    Args:
        index_file (Union[str, Path]): Chemin vers le fichier d'index JSON
        
    Returns:
        Dict[str, any]: Dictionnaire contenant tous les artefacts chargÃ©s
        
    Raises:
        FileNotFoundError: Si le fichier d'index n'existe pas
        OSError: Si impossible de charger les artefacts
        
    Examples:
        >>> artifacts = load_model_artifacts('models/fraud_detector_index_20250801_143000.json')
        >>> model = artifacts['model']
        >>> scaler = artifacts['scaler']
    """
    index_path = Path(index_file)
    
    if not index_path.exists():
        raise FileNotFoundError(f"Fichier d'index introuvable: {index_file}")
    
    # Charger l'index
    import json
    with open(index_path, 'r') as f:
        file_paths = json.load(f)
    
    artifacts = {}
    
    try:
        for artifact_type, file_path in file_paths.items():
            if artifact_type == 'index':
                continue
                
            with open(file_path, 'rb') as f:
                artifacts[artifact_type] = pickle.load(f)
                
        return artifacts
        
    except Exception as e:
        raise OSError(f"Erreur lors du chargement: {str(e)}")


def generate_pipeline_report(
    pipeline_results: Dict[str, any],
    output_file: Optional[Union[str, Path]] = None
) -> str:
    """
    GÃ©nÃ¨re un rapport dÃ©taillÃ© d'exÃ©cution du pipeline.
    
    CrÃ©e un rapport formatÃ© contenant toutes les mÃ©triques et rÃ©sultats
    d'une exÃ©cution complÃ¨te du pipeline de dÃ©tection de fraude.
    
    Args:
        pipeline_results (Dict[str, any]): RÃ©sultats du pipeline contenant
                                          les mÃ©triques de chaque Ã©tape
        output_file (Optional[Union[str, Path]], optional): Fichier de sortie.
                                                           Si None, retourne le contenu.
                                                           DÃ©faut: None
    
    Returns:
        str: Contenu du rapport formatÃ©
        
    Examples:
        >>> results = {
        ...     'data_validation': {'is_valid': True, 'rows': 50000},
        ...     'preprocessing': {'features_created': 8},
        ...     'synthetic_generation': {'samples_generated': 5000}
        ... }
        >>> report = generate_pipeline_report(results, 'reports/pipeline_report.md')
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report_lines = [
        "# Rapport d'ExÃ©cution du Pipeline de DÃ©tection de Fraude",
        f"**Date d'exÃ©cution:** {timestamp}",
        "",
        "## RÃ©sumÃ© ExÃ©cutif",
        ""
    ]
    
    # Ajouter les rÃ©sultats de chaque Ã©tape
    for step_name, step_results in pipeline_results.items():
        report_lines.extend([
            f"### {step_name.replace('_', ' ').title()}",
            ""
        ])
        
        if isinstance(step_results, dict):
            for key, value in step_results.items():
                report_lines.append(f"- **{key}:** {value}")
        else:
            report_lines.append(f"- **RÃ©sultat:** {step_results}")
        
        report_lines.append("")
    
    # Ajouter les recommandations
    report_lines.extend([
        "## Recommandations",
        "",
        "- Surveiller les mÃ©triques de performance en continu",
        "- Valider la qualitÃ© des donnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©es",
        "- Planifier le rÃ©entraÃ®nement selon la dÃ©rive observÃ©e",
        "",
        "---",
        f"*Rapport gÃ©nÃ©rÃ© automatiquement le {timestamp}*"
    ])
    
    report_content = "\n".join(report_lines)
    
    # Sauvegarder si fichier spÃ©cifiÃ©
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
    
    return report_content


# Configuration par dÃ©faut des chemins
DEFAULT_PATHS = {
    'raw_data': 'data/raw/',
    'processed_data': 'data/processed/',
    'synthetic_data': 'data/synthetic/',
    'models': 'models/',
    'logs': 'logs/',
    'reports': 'reports/'
}


def ensure_directory_structure(base_path: Union[str, Path] = ".") -> None:
    """
    CrÃ©e la structure de rÃ©pertoires nÃ©cessaire au projet.
    
    Args:
        base_path (Union[str, Path], optional): RÃ©pertoire racine du projet.
                                              DÃ©faut: rÃ©pertoire courant
    
    Examples:
        >>> ensure_directory_structure("/path/to/project")
    """
    base = Path(base_path)
    
    for path_name, relative_path in DEFAULT_PATHS.items():
        full_path = base / relative_path
        full_path.mkdir(parents=True, exist_ok=True)
        
        # CrÃ©er un fichier .gitkeep pour maintenir la structure dans Git
        gitkeep_file = full_path / '.gitkeep'
        if not gitkeep_file.exists():
            gitkeep_file.touch()


if __name__ == "__main__":
    # Tests basiques des fonctions
    print("ğŸ§ª Tests des fonctions utilitaires...")
    
    # Test de validation de dataset
    test_df = pd.DataFrame({
        'Time': [1, 2, 3, 4, 5],
        'Amount': [10.0, 20.0, 30.0, 40.0, 50.0],
        'Class': [0, 1, 0, 1, 0]
    })
    
    result = validate_dataset_quality(test_df, ['Time', 'Amount', 'Class'])
    print(f"âœ… Validation dataset: {result['is_valid']}")
    
    # Test de mÃ©triques de dÃ©sÃ©quilibre
    metrics = calculate_class_balance_metrics(test_df['Class'])
    print(f"âœ… Ratio de dÃ©sÃ©quilibre: {metrics['imbalance_ratio']:.1f}")
    
    # Test de structure de rÃ©pertoires
    ensure_directory_structure()
    print("âœ… Structure de rÃ©pertoires crÃ©Ã©e")
    
    print("ğŸ‰ Tous les tests sont passÃ©s!")